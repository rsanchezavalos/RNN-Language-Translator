{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "\n",
    "## Introduction\n",
    "- **Preprocess** - Convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on spanish text.\n",
    "\n",
    "## Dataset\n",
    "Datasets for machine translation from [WMT](http://www.statmt.org/).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load spanish data\n",
    "spanish_sentences = [line.rstrip('\\n') for line in open('data/eng.txt')]\n",
    "# Load english data\n",
    "english_sentences = [line.rstrip('\\n') for line in open('data/span.txt')]\n",
    "print('Dataset Loaded')\n",
    "\n",
    "# Filter - running out of memory\n",
    "spanish_sentences = spanish_sentences[10000:20000]\n",
    "english_sentences = english_sentences[10000:20000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line contains an spanish sentence with the respective translation in english.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  Can we trust Tom?\n",
      "small_vocab_fr Line 1:  ¿Podemos confiar en Tom?\n",
      "small_vocab_en Line 2:  Can we trust her?\n",
      "small_vocab_fr Line 2:  ¿Podemos confiar en ella?\n",
      "small_vocab_en Line 3:  Can you call him?\n",
      "small_vocab_fr Line 3:  ¿Podés llamarlo?\n",
      "small_vocab_en Line 4:  Can you call him?\n",
      "small_vocab_fr Line 4:  ¿Puedes llamarlo?\n",
      "small_vocab_en Line 5:  Can you find her?\n",
      "small_vocab_fr Line 5:  ¿Puedes encontrarla?\n",
      "small_vocab_en Line 6:  Can you find out?\n",
      "small_vocab_fr Line 6:  ¿Puedes descubrirlo?\n",
      "small_vocab_en Line 7:  Can you meet him?\n",
      "small_vocab_fr Line 7:  ¿Puedes reunirte con él?\n",
      "small_vocab_en Line 8:  Can you prove it?\n",
      "small_vocab_fr Line 8:  ¿Puedes demostrarlo?\n",
      "small_vocab_en Line 9:  Can you run fast?\n",
      "small_vocab_fr Line 9:  ¿Puedes correr rápido?\n",
      "small_vocab_en Line 10:  Can you see them?\n",
      "small_vocab_fr Line 10:  ¿Puedes verlos?\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(10):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, spanish_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38005 spanish words.\n",
      "4501 unique spanish words.\n",
      "10 Most common words in the spanish dataset:\n",
      "\"I\" \"Tom\" \"is\" \"a\" \"you\" \"He\" \"to\" \"the\" \"You\" \"I'm\"\n",
      "\n",
      "36647 english words.\n",
      "7446 unique english words.\n",
      "10 Most common words in the english dataset:\n",
      "\"es\" \"Tom\" \"a\" \"No\" \"un\" \"de\" \"Él\" \"la\" \"está\" \"el\"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "spanish_words_counter = collections.Counter([word for sentence in spanish_sentences for word in sentence.split()])\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} spanish words.'.format(len([word for sentence in spanish_sentences for word in sentence.split()])))\n",
    "print('{} unique spanish words.'.format(len(spanish_words_counter)))\n",
    "print('10 Most common words in the spanish dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*spanish_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} english words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique english words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the english dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `spanish_sentences` and `english_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'short': 20, 'of': 14, 'lexicography': 15, 'dog': 9, 'over': 7, 'quick': 2, 'my': 12, 'prize': 17, 'lazy': 8, 'sentence': 21, 'is': 19, 'jumps': 6, 'fox': 5, 'a': 3, 'the': 1, 'won': 16, 'by': 10, 'jove': 11, 'brown': 4, 'study': 13, 'this': 18}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    x_tk = Tokenizer(char_level=False,\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                     lower=True)\n",
    "    x_tk.fit_on_texts(x)\n",
    "\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding \n",
    "\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    seq = pad_sequences(x, maxlen=length,padding='post',value=0.0)\n",
    "    return seq\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_y, y_tk = tokenize(english_sentences)\n",
    "preprocess_y = pad(preprocess_y)\n",
    "preprocess_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max spanish sentence length: 6\n",
      "Max english sentence length: 12\n",
      "spanish vocabulary size: 3041\n",
      "english vocabulary size: 5459\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape((10000, 12, 1))\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_spanish_sentences, preproc_english_sentences, spanish_tokenizer, english_tokenizer =\\\n",
    "    preprocess(spanish_sentences, english_sentences)\n",
    "    \n",
    "max_spanish_sequence_length = preproc_spanish_sentences.shape[1]\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "spanish_vocab_size = len(spanish_tokenizer.word_index)\n",
    "english_vocab_size = len(english_tokenizer.word_index) \n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max spanish sentence length:\", max_spanish_sequence_length)\n",
    "print(\"Max english sentence length:\", max_english_sequence_length)\n",
    "print(\"spanish vocabulary size:\", spanish_vocab_size)\n",
    "print(\"english vocabulary size:\", english_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the english translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the english translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "A basic RNN model is a good baseline for sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 6s - loss: 8.5369 - acc: 0.4158 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 2s - loss: 8.2672 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 2s - loss: 7.7143 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2s - loss: 6.8745 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2s - loss: 5.8116 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2s - loss: 4.9332 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 2s - loss: 4.1732 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 2s - loss: 3.5452 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s - loss: 3.0868 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2s - loss: 2.8152 - acc: 0.6980 - val_loss: nan - val_acc: 0.6808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86b692feb8>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, spanish_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param spanish_vocab_size: Number of unique spanish words in the dataset\n",
    "    :param english_vocab_size: Number of unique english words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    learning_rate = 1e-3\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    model = LSTM(50, return_sequences=True)(inputs)\n",
    "    model = LSTM(50, return_sequences=True)(model)\n",
    "    model = TimeDistributed(Dense(english_vocab_size))(model)\n",
    "    model = Model(inputs, Activation('softmax')(model))\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])   \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_spanish_sentences, max_english_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_english_sequence_length,\n",
    "    spanish_vocab_size,\n",
    "    english_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_english_sentences,\n",
    "                     batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding\n",
    "An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape (None, 12), Output Shape (None, 12, 5460)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 12, 128)           389376    \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 12, 128)           98688     \n",
      "_________________________________________________________________\n",
      "time_distributed_24 (TimeDis (None, 12, 512)           66048     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 12, 5460)          2800980   \n",
      "=================================================================\n",
      "Total params: 3,355,092\n",
      "Trainable params: 3,355,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 7s - loss: 3.4798 - acc: 0.6882 - val_loss: 2.4468 - val_acc: 0.6808\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 4s - loss: 2.2371 - acc: 0.6980 - val_loss: 2.4075 - val_acc: 0.6808\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 4s - loss: 2.1594 - acc: 0.6990 - val_loss: 2.2770 - val_acc: 0.6991\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 4s - loss: 2.0362 - acc: 0.7099 - val_loss: 2.2068 - val_acc: 0.7121\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 4s - loss: 1.9736 - acc: 0.7187 - val_loss: 2.1591 - val_acc: 0.7170\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 4s - loss: 1.8950 - acc: 0.7264 - val_loss: 2.1109 - val_acc: 0.7219\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 4s - loss: 1.7992 - acc: 0.7344 - val_loss: 2.0453 - val_acc: 0.7282\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 4s - loss: 1.7067 - acc: 0.7409 - val_loss: 1.9913 - val_acc: 0.7390\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 4s - loss: 1.6268 - acc: 0.7482 - val_loss: 1.9639 - val_acc: 0.7420\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 4s - loss: 1.5543 - acc: 0.7531 - val_loss: 1.9540 - val_acc: 0.7457\n",
      "¿puedes lo en tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Original text and translation:\n",
      "['Can we trust Tom?']\n",
      "['¿Podemos confiar en Tom?']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, spanish_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param spanish_vocab_size: Number of unique spanish words in the dataset\n",
    "    :param english_vocab_size: Number of unique english words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    model = Sequential()\n",
    "    # output dim of embedding matrix tunable hyperparameter\n",
    "    model.add(Embedding(spanish_vocab_size, 128, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(512, activation='tanh')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax')))\n",
    "    learning_rate = 0.001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    print('Input Shape {}, Output Shape {}'.format(model.input_shape, model.output_shape))\n",
    "    return model\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_spanish_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2]))\n",
    "# TODO: Train the neural network\n",
    "# increased passed index length by 1 to avoid index error\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(spanish_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "print(embed_rnn_model.summary())\n",
    "# reduced batch size\n",
    "embed_rnn_model.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=10, validation_split=0.2)\n",
    "\n",
    "# TODO: Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], english_tokenizer))\n",
    "print(\"Original text and translation:\")\n",
    "print(spanish_sentences[:1])\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs (IMPLEMENTATION)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 12, 256)           99840     \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 12, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 12, 5460)          2800980   \n",
      "=================================================================\n",
      "Total params: 3,032,404\n",
      "Trainable params: 3,032,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 8s - loss: 3.0646 - acc: 0.6886 - val_loss: 2.3000 - val_acc: 0.6897\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 5s - loss: 2.1213 - acc: 0.7078 - val_loss: 2.2373 - val_acc: 0.7067\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 5s - loss: 2.0459 - acc: 0.7112 - val_loss: 2.1969 - val_acc: 0.7069\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.9791 - acc: 0.7148 - val_loss: 2.1816 - val_acc: 0.7125\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.9210 - acc: 0.7191 - val_loss: 2.1436 - val_acc: 0.7160\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.8603 - acc: 0.7225 - val_loss: 2.1179 - val_acc: 0.7203\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.8072 - acc: 0.7245 - val_loss: 2.0999 - val_acc: 0.7232\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.7609 - acc: 0.7265 - val_loss: 2.1103 - val_acc: 0.7241\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.7185 - acc: 0.7280 - val_loss: 2.0901 - val_acc: 0.7252\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.6771 - acc: 0.7303 - val_loss: 2.1021 - val_acc: 0.7261\n",
      "no tu tom <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Original text and translation:\n",
      "['Can we trust Tom?']\n",
      "['¿Podemos confiar en Tom?']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, spanish_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param spanish_vocab_size: Number of unique spanish words in the dataset\n",
    "    :param english_vocab_size: Number of unique english words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(512, activation='tanh')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax')))\n",
    "    learning_rate = 0.001   \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "# Reshaping the input\n",
    "tmp_x = pad(preproc_spanish_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "# increased passed index length by 1 to avoid index error\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(spanish_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "print(bd_rnn_model.summary())\n",
    "# reduced batch size\n",
    "bd_rnn_model.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], english_tokenizer))\n",
    "print(\"Original text and translation:\")\n",
    "print(spanish_sentences[:1])\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder \n",
    "This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_11 (GRU)                 (None, 128)               49920     \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 12, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 12, 128)           98688     \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, 12, 512)           66048     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, 12, 5460)          2800980   \n",
      "=================================================================\n",
      "Total params: 3,015,636\n",
      "Trainable params: 3,015,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 9s - loss: 2.9075 - acc: 0.6886 - val_loss: 2.3678 - val_acc: 0.6813\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 5s - loss: 2.1274 - acc: 0.7070 - val_loss: 2.2754 - val_acc: 0.6917\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 5s - loss: 2.0716 - acc: 0.7093 - val_loss: 2.2394 - val_acc: 0.6941\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 5s - loss: 2.0304 - acc: 0.7105 - val_loss: 2.2138 - val_acc: 0.7068\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.9898 - acc: 0.7113 - val_loss: 2.1955 - val_acc: 0.7045\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.9439 - acc: 0.7131 - val_loss: 2.1686 - val_acc: 0.7115\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.8923 - acc: 0.7165 - val_loss: 2.1468 - val_acc: 0.7114\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.8429 - acc: 0.7198 - val_loss: 2.1408 - val_acc: 0.7135\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.8016 - acc: 0.7220 - val_loss: 2.1274 - val_acc: 0.7166\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 5s - loss: 1.7580 - acc: 0.7245 - val_loss: 2.1253 - val_acc: 0.7189\n",
      "ella no <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Original text and translation:\n",
      "['Can we trust Tom?']\n",
      "['¿Podemos confiar en Tom?']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector\n",
    "\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, spanish_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param spanish_vocab_size: Number of unique spanish words in the dataset\n",
    "    :param english_vocab_size: Number of unique english words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    model = Sequential()\n",
    "    #Encoder\n",
    "    # reverse input sequence order for improved accuracy \n",
    "    model.add(GRU(128, input_shape=input_shape[1:], go_backwards=True))\n",
    "    #Adapter to fit 2D output to required 3D input shape [samples, time steps, features]\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    #Decoder\n",
    "    model.add(GRU(128, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(512, activation='tanh')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax')))\n",
    "    learning_rate = 0.001   \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# OPTIONAL: Train and Print prediction(s)\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_spanish_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "# increased passed index length by 1 to avoid index error\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(spanish_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "print(encdec_rnn_model.summary())\n",
    "# reduced batch size\n",
    "encdec_rnn_model.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], english_tokenizer))\n",
    "print(\"Original text and translation:\")\n",
    "print(spanish_sentences[:1])\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, RepeatVector, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def model_final(input_shape, output_sequence_length, spanish_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param spanish_vocab_size: Number of unique spanish words in the dataset\n",
    "    :param english_vocab_size: Number of unique english words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    model = Sequential()\n",
    "    #Embedding\n",
    "    model.add(Embedding(spanish_vocab_size, 128, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    #Bidirectional Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    #Adapter to fit 2D output to required GRU 3D input shape [samples, time steps, features]\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    #Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='tanh')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax')))\n",
    "    learning_rate = 0.001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 6, 128)            389376    \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_12 (RepeatVect (None, 12, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 12, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, 12, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, 12, 5460)          2800980   \n",
      "=================================================================\n",
      "Total params: 3,814,996\n",
      "Trainable params: 3,814,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "value_ctxt is not a CondContext: <tensorflow.python.ops.control_flow_ops.WhileContext object at 0x7f869eceab00>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1518\u001b[0m       raise ValueError(\"No attr named '\" + name + \"' in \" +\n\u001b[0;32m-> 1519\u001b[0;31m                        str(self._node_def))\n\u001b[0m\u001b[1;32m   1520\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No attr named '_XlaCompile' in name: \"dropout_16/cond/Merge\"\nop: \"Merge\"\ninput: \"dropout_16/cond/Switch_1\"\ninput: \"dropout_16/cond/dropout/mul\"\nattr {\n  key: \"N\"\n  value {\n    i: 2\n  }\n}\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-cfe5505c127a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproc_spanish_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_english_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspanish_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-211-cfe5505c127a>\u001b[0m in \u001b[0;36mfinal_predictions\u001b[0;34m(x, y, x_tk, y_tk)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# reduced batch size to 100 and train for 20 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0my_id_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_tk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    959\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    961\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                 \u001b[0;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clipnorm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclipnorm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.0.8-py3.4.egg/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2308\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m     \"\"\"\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[0;32m--> 560\u001b[0;31m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    561\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 in_grads = _MaybeCompile(\n\u001b[0;32m--> 560\u001b[0;31m                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    561\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_grad.py\u001b[0m in \u001b[0;36m_MergeGrad\u001b[0;34m(op, grad, _)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mgrad_ctxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mgrad_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mhistory_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddForwardAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mgrad_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAddForwardAccumulator\u001b[0;34m(self, value, dead_branch)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_ctxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCondContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m           raise TypeError(\n\u001b[0;32m--> 878\u001b[0;31m               \"value_ctxt is not a CondContext: %s\" % value_ctxt)\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdead_branch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m           \u001b[0;31m# The special case for creating a zero tensor for a dead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: value_ctxt is not a CondContext: <tensorflow.python.ops.control_flow_ops.WhileContext object at 0x7f869eceab00>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed spanish data\n",
    "    :param y: Preprocessed english data\n",
    "    :param x_tk: spanish tokenizer\n",
    "    :param y_tk: english tokenizer\n",
    "    \"\"\"\n",
    "    # increased passed index length by 1 to avoid index error\n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    print(model.summary())\n",
    "    # reduced batch size to 100 and train for 20 epochs\n",
    "    model.fit(x, y, batch_size=100, epochs=10, validation_split=0.2)\n",
    "\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'él tiene un diario'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "final_predictions(preproc_spanish_sentences, preproc_english_sentences, spanish_tokenizer, english_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.4/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 73.80%\n",
      "acc: 74.23%\n",
      "acc: 73.71%\n",
      "73.91% (+/- 0.23%)\n"
     ]
    }
   ],
   "source": [
    "# validation_split argument in model.fit holds out the last 20% of the training data set\n",
    "# validation accuracy should be verified over different data sets\n",
    "# try stratfied Kfold cross validation as described in:\n",
    "    # https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "# fixed random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# define 3-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "cvscores=[]\n",
    "for train, test in kfold.split(preproc_spanish_sentences,preproc_english_sentences[:,0,0]):\n",
    "    # compile model\n",
    "    model = model_final(preproc_spanish_sentences[train].shape,\n",
    "                        preproc_english_sentences[train].shape[1],\n",
    "                        len(spanish_tokenizer.word_index)+1,\n",
    "                        len(english_tokenizer.word_index)+1)\n",
    "    # fit the model - 10 epochs\n",
    "    model.fit(preproc_spanish_sentences[train], preproc_english_sentences[train],\n",
    "              batch_size=100, epochs=10, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(preproc_spanish_sentences[test], preproc_english_sentences[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1]*100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation accuracy with 3-fold CV (for 10 epochs) is quite consistent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
